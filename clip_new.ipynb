{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ccx_yI-Ctqrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8134b289-9e04-4e46-867e-920dba5d8670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-nro_wskk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-nro_wskk\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers pillow tqdm matplotlib\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def download_flickr8k():\n",
        "    print(\"Downloading Flickr8k images...\")\n",
        "    image_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
        "    response = requests.get(image_url)\n",
        "    with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    print(\"Downloading Flickr8k captions...\")\n",
        "    caption_url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
        "    response = requests.get(caption_url)\n",
        "    with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "def custom_collate(batch):\n",
        "    images = torch.stack([item['image'] for item in batch])\n",
        "    image_raw = [item['image_raw'] for item in batch]\n",
        "    captions = [item['captions'] for item in batch]\n",
        "    image_ids = [item['image_id'] for item in batch]\n",
        "    image_paths = [item['image_path'] for item in batch]\n",
        "\n",
        "    return {\n",
        "        'image': images,\n",
        "        'image_raw': image_raw,\n",
        "        'captions': captions,\n",
        "        'image_id': image_ids,\n",
        "        'image_path': image_paths\n",
        "    }\n",
        "\n",
        "class Flickr8kDataset(Dataset):\n",
        "    def __init__(self, image_dir, captions_file, transform=None):\n",
        "      self.image_dir = image_dir\n",
        "      self.transform = transform\n",
        "      self.image_captions = {}\n",
        "      self.image_paths = []\n",
        "\n",
        "      with open(captions_file, 'r', encoding='utf-8') as f:\n",
        "          for line in f:\n",
        "              parts = line.strip().split('\\t')\n",
        "              if len(parts) != 2:\n",
        "                  continue\n",
        "\n",
        "              image_name = parts[0].split('#')[0]\n",
        "              if '.jpg.1' in image_name:\n",
        "                  image_name = image_name.replace('.jpg.1', '.jpg')\n",
        "              caption = parts[1].strip()\n",
        "\n",
        "              image_path = os.path.join('/content', self.image_dir, image_name)\n",
        "\n",
        "              if image_name in self.image_captions:\n",
        "                  self.image_captions[image_name].append(caption)\n",
        "              else:\n",
        "                  self.image_captions[image_name] = [caption]\n",
        "                  self.image_paths.append(image_path)\n",
        "\n",
        "      self.image_paths = list(dict.fromkeys(self.image_paths))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image_name = os.path.basename(image_path)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "        if self.transform:\n",
        "            image_tensor = self.transform(image)\n",
        "\n",
        "        return {\n",
        "            'image': image_tensor,\n",
        "            'image_raw': image,\n",
        "            'captions': self.image_captions[image_name],\n",
        "            'image_id': idx,\n",
        "            'image_path': image_path\n",
        "        }\n",
        "\n",
        "class CLIPRetrieval:\n",
        "    def __init__(self, device='cuda'):\n",
        "        self.device = device\n",
        "        self.model, self.preprocess = clip.load('ViT-B/32', device)\n",
        "        self.dataset = None\n",
        "        self.image_features = None\n",
        "        self.all_images = []\n",
        "\n",
        "    def load_dataset(self, image_dir, captions_file):\n",
        "        self.dataset = Flickr8kDataset(image_dir, captions_file, self.preprocess)\n",
        "        dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            collate_fn=custom_collate\n",
        "        )\n",
        "\n",
        "        image_features = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Processing images\"):\n",
        "                images = batch['image'].to(self.device)\n",
        "                self.all_images.extend(batch['image_raw'])\n",
        "\n",
        "                batch_image_features = self.model.encode_image(images)\n",
        "                batch_image_features = F.normalize(batch_image_features, dim=1)\n",
        "                image_features.append(batch_image_features)\n",
        "\n",
        "        self.image_features = torch.cat(image_features)\n",
        "\n",
        "    def evaluate_recalls(self):\n",
        "        if self.dataset is None:\n",
        "            raise ValueError(\"Dataset not loaded. Call load_dataset first.\")\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            self.dataset,\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            collate_fn=custom_collate\n",
        "        )\n",
        "        text_features = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dataloader, desc=\"Processing text\"):\n",
        "                captions = [caps[0] for caps in batch['captions']]\n",
        "                text_tokens = clip.tokenize(captions).to(self.device)\n",
        "                batch_text_features = self.model.encode_text(text_tokens)\n",
        "                batch_text_features = F.normalize(batch_text_features, dim=1)\n",
        "                text_features.append(batch_text_features)\n",
        "\n",
        "        text_features = torch.cat(text_features)\n",
        "        similarity = torch.matmul(text_features, self.image_features.t())\n",
        "\n",
        "        results = calculate_recalls(similarity)\n",
        "        print(\"\\nCLIP Recall@K Results:\")\n",
        "        for k, v in results.items():\n",
        "            print(f\"{k}: {v:.2f}%\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def retrieve_images(self, query_caption, k=5):\n",
        "        with torch.no_grad():\n",
        "            text_tokens = clip.tokenize([query_caption]).to(self.device)\n",
        "            text_features = self.model.encode_text(text_tokens)\n",
        "            text_features = F.normalize(text_features, dim=1)\n",
        "\n",
        "            similarity = torch.matmul(text_features, self.image_features.t())[0]\n",
        "            top_k_scores, top_k_indices = similarity.topk(k)\n",
        "\n",
        "            retrieved_results = []\n",
        "            for score, idx in zip(top_k_scores.cpu(), top_k_indices.cpu()):\n",
        "                retrieved_results.append((self.all_images[idx], score.item()))\n",
        "\n",
        "            return retrieved_results\n",
        "\n",
        "def calculate_recalls(similarity_matrix, k_values=[1, 5, 10]):\n",
        "    results = {}\n",
        "    num_samples = similarity_matrix.shape[0]\n",
        "\n",
        "\n",
        "    ground_truth = torch.arange(num_samples).to(similarity_matrix.device)\n",
        "    ranks = torch.where(torch.argsort(similarity_matrix, dim=1, descending=True) == ground_truth.unsqueeze(1))[1]\n",
        "\n",
        "    for k in k_values:\n",
        "        recall_k = (ranks < k).float().mean().item()\n",
        "        results[f'R@{k}'] = recall_k * 100\n",
        "\n",
        "    return results\n",
        "\n",
        "def find_image_for_caption(caption_file, target_caption):\n",
        "\n",
        "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split('\\t')\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            image_name = parts[0].split('#')[0]\n",
        "            if '.jpg.1' in image_name:\n",
        "                image_name = image_name.replace('.jpg.1', '.jpg')\n",
        "            caption = parts[1].strip()\n",
        "\n",
        "            if caption.lower() == target_caption.lower():\n",
        "                return image_name\n",
        "    return None\n",
        "\n",
        "def visualize_retrieval_results(query_caption, top_5_images, save_path='retrieval_results.png', caption_file='/content/Flickr8k.token.txt'):\n",
        "    image_name = find_image_for_caption(caption_file, query_caption)\n",
        "    if image_name:\n",
        "        try:\n",
        "            original_image = Image.open(os.path.join('/content/Flickr8k_Dataset', image_name)).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load original image: {e}\")\n",
        "            original_image = None\n",
        "    else:\n",
        "        print(\"Warning: Could not find original image for the caption\")\n",
        "        original_image = None\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    fig.suptitle(f'Query: \"{query_caption}\"', fontsize=12, wrap=True)\n",
        "\n",
        "    if original_image:\n",
        "        axes[0, 0].imshow(original_image)\n",
        "        axes[0, 0].set_title('Original Image', color='red')\n",
        "    else:\n",
        "        axes[0, 0].text(0.5, 0.5, 'Original image\\nnot found',\n",
        "                       ha='center', va='center', color='red')\n",
        "    axes[0, 0].axis('off')\n",
        "\n",
        "    for idx, (img, score) in enumerate(top_5_images):\n",
        "        row = (idx + 1) // 3\n",
        "        col = (idx + 1) % 3\n",
        "        axes[row, col].imshow(img)\n",
        "        axes[row, col].axis('off')\n",
        "        axes[row, col].set_title(f'Score: {score:.3f}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "    return save_path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('Flickr8k_Dataset'):\n",
        "  download_flickr8k()\n",
        "\n",
        "  print(\"\\nCurrent directory structure:\")\n",
        "  os.system('ls -R Flickr8k_Dataset')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cQaIAX6utAg",
        "outputId": "0e947a02-d6a2-4e72-9184-78a30b1b1380"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Flickr8k images...\n",
            "Downloading Flickr8k captions...\n",
            "\n",
            "Current directory structure:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "clip_retrieval = CLIPRetrieval(device)\n",
        "\n",
        "clip_retrieval.load_dataset('Flickr8k_Dataset', '/content/Flickr8k.token.txt')\n",
        "\n",
        "clip_retrieval.evaluate_recalls()\n",
        "\n",
        "query_caption = \"a dog playing in the water\"\n",
        "retrieved_images = clip_retrieval.retrieve_images(query_caption, k=5)\n",
        "\n",
        "result_path = visualize_retrieval_results(query_caption, retrieved_images)\n",
        "print(f\"\\nRetrieval results visualization saved to: {result_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmAmNJLzG-1_",
        "outputId": "cf378c39-fca7-4e74-a5da-9c172c9d1155"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing images: 100%|██████████| 253/253 [01:10<00:00,  3.59it/s]\n",
            "Processing text: 100%|██████████| 253/253 [01:15<00:00,  3.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CLIP Recall@K Results:\n",
            "R@1: 29.80%\n",
            "R@5: 53.03%\n",
            "R@10: 63.14%\n",
            "Warning: Could not load original image: [Errno 2] No such file or directory: '/content/Flickr8k_Dataset/2744705574_519c171ca0.jpg'\n",
            "\n",
            "Retrieval results visualization saved to: retrieval_results.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_caption = \"A girl going into a wooden building .\"\n",
        "retrieved_images = clip_retrieval.retrieve_images(query_caption, k=5)\n",
        "\n",
        "result_path = visualize_retrieval_results(query_caption, retrieved_images)\n",
        "print(f\"\\nRetrieval results visualization saved to: {result_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqy9e3MA1snX",
        "outputId": "2319ce2d-33e3-42aa-f9ab-e4469ff0bad8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieval results visualization saved to: retrieval_results.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83QZ2G2y1uaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}