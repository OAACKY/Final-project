# -*- coding: utf-8 -*-
"""VSE++.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vwp1kbBUGOS55bYs_VHK3oYelB-HC8oL
"""

!pip install kagglehub torch torchvision transformers

import os
import kagglehub
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
import nltk
from PIL import Image
from transformers import BertTokenizer, BertModel
import json

nltk.download('punkt')

# Download the Flickr8K dataset using kagglehub
path = kagglehub.dataset_download("adityajn105/flickr8k")
print("Path to dataset files:", path)

# Define paths to images and captions
images_path = os.path.join(path, "Images")
captions_file = os.path.join(path, "captions.txt")

print("Images Path:", images_path)
print("Captions File:", captions_file)

# Load captions data
captions_dict = {}
with open(captions_file, 'r') as f:
    next(f)  # Skip header
    for line in f:
        image_name, caption = line.strip().split(',', 1)
        if image_name not in captions_dict:
            captions_dict[image_name] = []
        captions_dict[image_name].append(caption)

print("Total Images in Dataset:", len(captions_dict))

class Flickr8kDataset(Dataset):
    def __init__(self, images_path, captions_dict, transform=None):
        self.images_path = images_path
        self.image_caption_pairs = [
            (img, caption) for img, captions in captions_dict.items() for caption in captions
        ]
        self.transform = transform

    def __len__(self):
        return len(self.image_caption_pairs)

    def __getitem__(self, idx):
        img_name, caption = self.image_caption_pairs[idx]
        img_path = os.path.join(self.images_path, img_name)
        image = Image.open(img_path).convert("RGB")
        if self.transform:
            image = self.transform(image)
        return image, caption

# Define image transformations
transform = transforms.Compose([
    transforms.Resize((250, 250)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Initialize Dataset and DataLoader
dataset = Flickr8kDataset(images_path, captions_dict, transform=transform)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

dataloader = DataLoader(
    dataset,
    batch_size=64,
    shuffle=True,
    num_workers=64,
    pin_memory=True
)

# Test DataLoader
for images, captions in dataloader:
    print("Image batch shape:", images.shape)
    print("Example Captions:", captions[:2])
    break

import torchvision.models as models

class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        base_model = models.resnet50(pretrained=True)  # Use pretrained ResNet50
        self.cnn = nn.Sequential(*list(base_model.children())[:-1])  # Remove classification layer

    def forward(self, x):
        x = self.cnn(x)
        return x.view(x.size(0), -1)  # Flatten output

class VSEModel(nn.Module):
    def __init__(self, embed_dim):
        super(VSEModel, self).__init__()
        self.encoder = Encoder()
        self.text_embed = nn.Linear(768, embed_dim)
        self.image_embed = nn.Linear(2048, embed_dim)

    def forward(self, image, text):
        img_features = self.encoder(image)
        img_embed = self.image_embed(img_features)
        text_embed = self.text_embed(text)
        return img_embed, text_embed

# Instantiate the model
model = VSEModel(embed_dim=512).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
print(model)

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
text_encoder = BertModel.from_pretrained("bert-base-uncased").to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

def get_text_embedding(captions):
    tokens = tokenizer(captions, padding=True, truncation=True, return_tensors="pt").to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    with torch.no_grad():
        embeddings = text_encoder(**tokens).last_hidden_state.mean(dim=1)
    return embeddings  # Shape: (batch_size, embed_dim)

# Define Loss and Optimizer
criterion = nn.TripletMarginLoss(margin=1.0)
optimizer = optim.Adam(model.parameters(), lr=0.0003)

def train_model(model, dataloader, criterion, optimizer, num_epochs=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        for images, captions in dataloader:
            images = images.to(device)
            captions_embed = get_text_embedding(captions).to(device)

            optimizer.zero_grad()
            img_embed, txt_embed = model(images, captions_embed)

            # Hard negative mining: select hardest negatives
            distances = torch.cdist(img_embed, txt_embed)
            _, hard_neg_indices = torch.topk(distances, k=2, largest=False)
            hard_neg_embed = txt_embed[hard_neg_indices[:, 1]]

            loss = criterion(img_embed, txt_embed, hard_neg_embed)

            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}")

# Start training
train_model(model, dataloader, criterion, optimizer)

# Save the model
torch.save(model.state_dict(), "vse_model.pth")

model = VSEModel(embed_dim=512).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))

# Load the model
model.load_state_dict(torch.load("vse_model.pth"))
print("Model Loaded Successfully!")

# Evaluation placeholder
model.eval()
example_image, example_caption = dataset[0]
example_image = example_image.unsqueeze(0).to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
example_caption_embed = get_text_embedding([example_caption])

with torch.no_grad():
    image_embedding, text_embedding = model(example_image, example_caption_embed)

from torch.nn.functional import cosine_similarity
similarity = cosine_similarity(image_embedding, text_embedding)
print("Cosine Similarity:", similarity.item())

import torch
from torch.nn.functional import cosine_similarity
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.eval()

def compute_image_embeddings(model, images_path, captions_dict, transform):
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    image_embeds = []
    image_names = []
    with torch.no_grad():
        for img_name in captions_dict.keys():
            img_path = os.path.join(images_path, img_name)
            image = Image.open(img_path).convert("RGB")
            image = transform(image).unsqueeze(0).to(device)

            dummy_text = torch.zeros((1, model.text_embed.in_features), device=device)
            img_embed, _ = model(image, dummy_text)
            image_embeds.append(img_embed.cpu().numpy())
            image_names.append(img_name)
    image_embeds = np.vstack(image_embeds)
    return image_embeds, image_names

image_embeddings, image_names = compute_image_embeddings(model, images_path, captions_dict, transform)

def retrieve_images_from_text(model, query, image_embeddings, image_names, top_k=5):
    text_embed = get_text_embedding([query])
    model.eval()
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    with torch.no_grad():
        dummy_image = torch.zeros((1,3,224,224), device=device)
        text_embed = text_embed.to(device)
        _, text_embed_mapped = model(dummy_image, text_embed)

        image_embeddings_tensor = torch.tensor(image_embeddings, device=device)
        similarities = cosine_similarity(text_embed_mapped, image_embeddings_tensor)
        similarities = similarities.cpu().numpy().flatten()

    sorted_indices = np.argsort(similarities)[::-1]

    seen = set()
    unique_top_images = []
    unique_top_scores = []
    for idx in sorted_indices:
        img_name = image_names[idx]
        if img_name not in seen:
            seen.add(img_name)
            unique_top_images.append(img_name)
            unique_top_scores.append(similarities[idx])
        if len(unique_top_images) == top_k:
            break

    return unique_top_images, unique_top_scores

query_image = image_names[413]
query = captions_dict[query_image][3]
top_images, top_scores = retrieve_images_from_text(model, query, image_embeddings, image_names, top_k=5)

print("Query:", query)

num_images = len(top_images) + 1  # Total number of images, including the Query Image
cols = 3  # Number of columns in the grid
rows = (num_images + cols - 1) // cols
match_count = [1, 2, 3, 4, 5]

# Create subplots
fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
axes = axes.flatten()

# 1. Display the Query Image
query_image_path = os.path.join(images_path, query_image)
query_img = Image.open(query_image_path).convert("RGB")
axes[0].imshow(query_img)
axes[0].set_title("Query Image\n", fontsize=12, loc="center")
axes[0].axis("off")

# 2. Display retrieved images
for i, (img_name, score, match) in enumerate(zip(top_images, top_scores, match_count)):
    img_path = os.path.join(images_path, img_name)
    img = Image.open(img_path).convert("RGB")

    axes[i + 1].imshow(img)
    axes[i + 1].set_title(f"Match {match}\nScore: {score:.4f}", fontsize=10, loc="center")
    axes[i + 1].axis("off")

# Remove unused subplots
for i in range(num_images, len(axes)):
    fig.delaxes(axes[i])

# Adjust layout to avoid overlapping titles and images
plt.tight_layout()
plt.show()

# Compute Recall@K
image_text_map = {}
for img_name, captions in captions_dict.items():
    combined_caption = " [SEP] ".join(captions)
    image_text_map[img_name] = combined_caption

image_embeddings, unique_image_names = compute_image_embeddings(model, images_path, captions_dict, transform)

img_name_to_index = {name: i for i, name in enumerate(unique_image_names)}

def compute_recall_at_k(model, image_embeddings, image_names, image_text_map, k=5):
    hits = 0
    total = len(image_text_map)
    for img_name, combined_text in image_text_map.items():
        retrieved_images, _ = retrieve_images_from_text(model, combined_text, image_embeddings, image_names, top_k=k)
        if img_name in retrieved_images:
            hits += 1
    recall_k = hits / total
    return recall_k

recall_1 = compute_recall_at_k(model, image_embeddings, unique_image_names, image_text_map, k=1)
print(f"Recall@1: {recall_1:.4f}")

recall_5 = compute_recall_at_k(model, image_embeddings, unique_image_names, image_text_map, k=5)
print(f"Recall@5: {recall_5:.4f}")

recall_10 = compute_recall_at_k(model, image_embeddings, unique_image_names, image_text_map, k=10)
print(f"Recall@10: {recall_10:.4f}")